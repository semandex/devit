{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-02T00:40:12.551319900Z",
     "start_time": "2023-11-02T00:40:12.527213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IRandman\\Documents\\devit\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%pwd\n",
    "from demo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def main(\n",
    "        # config_file=\"../configs/open-vocabulary/lvis/vitl.yaml\",\n",
    "        config_file=\"configs/few-shot/vitl_shot5.yaml\",\n",
    "        # rpn_config_file=\"../configs/RPN/mask_rcnn_R_50_FPN_1x.yaml\",\n",
    "        rpn_config_file=\"configs/RPN/mask_rcnn_R_50_C4_1x_fewshot_14.yaml\",\n",
    "        # model_path=\"../weights/trained/open-vocabulary/lvis/vitl_0069999.pth\",\n",
    "        model_path=\"weights/trained/few-shot/vitl_0089999.pth\",\n",
    "\n",
    "        image_dir='demo/input/input_boat',\n",
    "        output_dir='demo/output',\n",
    "        category_space=\"demo/boat_prototypes.pth\",\n",
    "        device='cuda:0',\n",
    "        overlapping_mode=True,\n",
    "        topk=1,\n",
    "        output_pth=False,\n",
    "        threshold=0.45\n",
    "    ):\n",
    "    assert osp.abspath(image_dir) != osp.abspath(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    config = get_cfg()\n",
    "    config.merge_from_file(config_file)\n",
    "    config.DE.OFFLINE_RPN_CONFIG = rpn_config_file\n",
    "    config.DE.TOPK = topk\n",
    "    config.MODEL.MASK_ON = False\n",
    "\n",
    "    config.freeze()\n",
    "    \n",
    "    augs = utils.build_augmentation(config, False)\n",
    "    augmentations = T.AugmentationList(augs) \n",
    "\n",
    "    # building models\n",
    "    model = Trainer.build_model(config).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device)['model'])\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    if category_space is not None:\n",
    "        category_space = torch.load(category_space)\n",
    "\n",
    "        if len(category_space[\"label_names\"]) < 2:\n",
    "            category_space[\"label_names\"].append(\"_blank_\")\n",
    "            real_prototypes = category_space[\"prototypes\"]\n",
    "            blank_prototypes = torch.zeros(1, 1024)\n",
    "            category_space[\"prototypes\"] = torch.cat((real_prototypes, blank_prototypes), 0)\n",
    "        model.label_names = category_space[\"label_names\"]\n",
    "        model.test_class_weight = category_space[\"prototypes\"].to(device)\n",
    "        \n",
    "    label_names =  model.label_names\n",
    "    if 'mini soccer' in label_names: # for YCB\n",
    "        label_names = list_replace(label_names, old='mini soccer', new='ball')\n",
    "\n",
    "    for img_file in tqdm(glob(osp.join(image_dir, '*'))):\n",
    "        base_filename = osp.splitext(osp.basename(img_file))[0]\n",
    "\n",
    "        dataset_dict = {}\n",
    "        image = utils.read_image(img_file, format=\"RGB\")\n",
    "        dataset_dict[\"height\"], dataset_dict[\"width\"] = image.shape[0], image.shape[1]\n",
    "\n",
    "        aug_input = T.AugInput(image)\n",
    "        augmentations(aug_input)\n",
    "        dataset_dict[\"image\"] = torch.as_tensor(np.ascontiguousarray(aug_input.image.transpose(2, 0, 1))).to(device)\n",
    "\n",
    "        batched_inputs = [dataset_dict]\n",
    "\n",
    "        output = model(batched_inputs)[0]\n",
    "        output['label_names'] = model.label_names\n",
    "        if output_pth:\n",
    "            torch.save(output, osp.join(output_dir, base_filename + '.pth'))\n",
    "\n",
    "        # visualize output\n",
    "        instances = output['instances']\n",
    "        boxes, pred_classes, scores = filter_boxes(instances, threshold=threshold)\n",
    "\n",
    "        if overlapping_mode:\n",
    "            # remove some highly overlapped predictions\n",
    "            mask = box_area(boxes) >= 400\n",
    "            boxes = boxes[mask]\n",
    "            pred_classes = pred_classes[mask]\n",
    "            scores = scores[mask]\n",
    "            mask = ops.nms(boxes, scores, 0.3)\n",
    "            boxes = boxes[mask]\n",
    "            pred_classes = pred_classes[mask]\n",
    "            scores = scores[mask]\n",
    "            areas = box_area(boxes)\n",
    "            indexes = list(range(len(pred_classes)))\n",
    "            for c in torch.unique(pred_classes).tolist():\n",
    "                box_id_indexes = (pred_classes == c).nonzero().flatten().tolist()\n",
    "                for i in range(len(box_id_indexes)):\n",
    "                    for j in range(i+1, len(box_id_indexes)):\n",
    "                        bid1 = box_id_indexes[i]\n",
    "                        bid2 = box_id_indexes[j]\n",
    "                        arr1 = boxes[bid1].cpu().numpy()\n",
    "                        arr2 = boxes[bid2].cpu().numpy()\n",
    "                        a1 = np.prod(arr1[2:] - arr1[:2])\n",
    "                        a2 = np.prod(arr2[2:] - arr2[:2])\n",
    "                        top_left = np.maximum(arr1[:2], arr2[:2]) # [[x, y]]\n",
    "                        bottom_right = np.minimum(arr1[2:], arr2[2:]) # [[x, y]]\n",
    "                        wh = bottom_right - top_left\n",
    "                        ia = wh[0].clip(0) * wh[1].clip(0)\n",
    "                        if ia >= 0.9 * min(a1, a2): # same class overlapping case, and larger one is much larger than small\n",
    "                            if a1 >= a2:\n",
    "                                if bid2 in indexes:\n",
    "                                    indexes.remove(bid2)\n",
    "                            else:\n",
    "                                if bid1 in indexes:\n",
    "                                    indexes.remove(bid1)\n",
    "\n",
    "            boxes = boxes[indexes]\n",
    "            pred_classes = pred_classes[indexes]\n",
    "        colors = assign_colors(pred_classes, label_names, seed=4)\n",
    "        output = to_pil_image(draw_bounding_boxes(torch.as_tensor(image).permute(2, 0, 1), boxes, labels=[f'{label_names[cid]}_{score:.2f}' for cid, score in zip(pred_classes.tolist(), scores.tolist())], colors=colors))\n",
    "        output.save(osp.join(output_dir, base_filename + '.out.jpg'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T00:40:29.319861Z",
     "start_time": "2023-11-02T00:40:29.316880400Z"
    }
   },
   "id": "7ed3efb1f7a03693"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]C:\\Users\\IRandman\\.conda\\envs\\devit\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "C:\\Users\\IRandman\\AppData\\Local\\Temp\\ipykernel_26652\\2027459221.py:112: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:205.)\n",
      "  output = to_pil_image(draw_bounding_boxes(torch.as_tensor(image).permute(2, 0, 1), boxes, labels=[f'{label_names[cid]}_{score:.2f}' for cid, score in zip(pred_classes.tolist(), scores.tolist())], colors=colors))\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T00:40:37.544614800Z",
     "start_time": "2023-11-02T00:40:29.473993600Z"
    }
   },
   "id": "78ce5f178cfc9f50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8cc62d739941f9e9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
